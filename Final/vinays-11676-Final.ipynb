{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "For this topic visualization project I will be using the Gensim libraries (https://radimrehurek.com/gensim/) and the D3 visualization library (http://dpoetry.com/theplains/Hierarchie-gh-pages/). I will be implementing the TFIDF concept to calculate how important a particular word is in a document. I will store the TFIDF value against the word and document. Based on the input word, I will find all the documents that word is present in and then retrieve all the important words present in those documents. I will then accumalate the TFIDF value of each of those words and then retrieve the top 5 words among them. These 5 words will represent the 5 most relevant words of the input word.\n",
    "\n",
    "## Steps\n",
    "The project can be implemented in 4 steps\n",
    "1. Preprocess the data\n",
    "2. Create a TFIDF market matrix\n",
    "3. Generate the json based on the input term\n",
    "4. Create the visualization app\n",
    "\n",
    "## Step 1\n",
    "I downloaded the PMC data in plain text format.\n",
    "I was able to read the data from files into an array of documents.\n",
    "I also removed any special characters from each document using a regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    documents = []\n",
    "    import glob\n",
    "    import os\n",
    "    directoryNames = list(set(glob.glob(os.path.join(\"Data\", \"*\"))).difference(set(glob.glob(os.path.join(\"Data\",\"*.*\")))))\n",
    "    numberOfDocuments = 0\n",
    "\n",
    "    for folder in directoryNames:\n",
    "        for fileNameDir in os.walk(folder):\n",
    "            for fileName in fileNameDir[2]:\n",
    "                if fileName[-4:] != \".txt\":\n",
    "                    continue\n",
    "                nameFileDocument = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)\n",
    "                with open(nameFileDocument, 'r') as doc:\n",
    "                    doc_text = doc.read().replace('\\n', '')\n",
    "                import re\n",
    "                processed_doc_text = re.sub('[^a-zA-Z0-9\\n]', ' ', doc_text)\n",
    "                documents.append(processed_doc_text)\n",
    "                numberOfDocuments += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then proceeded to tokennize each document and removed all the english \"stopwords\". \n",
    "I got the list of stop words from the stop-words package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # remove common words and tokenize\n",
    "    from stop_words import get_stop_words\n",
    "    stop_words = get_stop_words('english')\n",
    "    texts = [[word for word in document.lower().split() if word not in stop_words]\n",
    "             for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then proceeded to remove all the words that appeared only once in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # remove words that appear only once\n",
    "    from collections import defaultdict\n",
    "    frequency = defaultdict(int)\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            frequency[token] += 1\n",
    "\n",
    "    texts = [[token for token in text if frequency[token] > 1]\n",
    "             for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Since I had decided to use gensim libraries for this project, for creating a TFIDF market matrix, I need a dictionary and a corpus (bag of words). \n",
    "I will create a dictionary from the list of documents using the Gensim API.\n",
    "I will also be using the \"filter_extremes\" function of the gensim dictionary object to filter out the tokens that appear in,\n",
    "1. less than no_below documents (absolute number) or\n",
    "2. more than no_above documents (fraction of total corpus size, not absolute number).\n",
    "3. after (1) and (2), keep only the first keep_n most frequent tokens (or keep all if None)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    from gensim import corpora\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=20, no_above=0.1, keep_n=1000000)\n",
    "    dictionary.save('files/pmc-data.dict') # store the dictionary, for future reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create the corpus object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    corpora.MmCorpus.serialize('files/pmc-data.mm', corpus) # store to disk, for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create the TFIDF market matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    from gensim.corpora import MmCorpus\n",
    "    mm = MmCorpus('files/pmc-data.mm')\n",
    "    from gensim.models import TfidfModel\n",
    "    tfidf = TfidfModel(mm, id2word=dictionary, normalize=True)\n",
    "    MmCorpus.serialize('files/pmc-data-tfidf.mm', tfidf[mm], progress_cnt=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Now since the TFIDF matrix is ready, I can not take the use input and then create a json response for visualizing the relevant words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Ask user to input the term\n",
    "    term = input(\"Please input the term you want to visualize: \")\n",
    "    if term != \"\":\n",
    "        # generate the json tree of relevant words for the input term\n",
    "        response = generateJSON(term, 1, [])\n",
    "        import json\n",
    "        with open('/var/www/Hierarchie/app/data/pmc-data.json', 'w') as outfile:\n",
    "            json.dump(response, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also load the prevously saved data dictionary and tfidf data. \n",
    "I will create a id2word and word2id hashes from the dictionary object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionary from the file. Create id2Word and word2Id variables\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary.load('files/pmc-data.dict')\n",
    "id2word = dictionary.token2id\n",
    "word2id = {v: k for k, v in id2word.items()}\n",
    "\n",
    "# Load the tfidf data\n",
    "from scipy.io import mmread\n",
    "file = mmread('files/pmc-data-tfidf.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now ready to create the json. \n",
    "I will create a seperate function that does this job. \n",
    "First for finding the relevant words of the input term, I need to do 2 things. \n",
    "1. Find all the documents that have the input word\n",
    "2. Get all the words in the documents that have the input word\n",
    "I write this logic in 2 util functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrives all the doc IDs in the TFIDF that contain the word\n",
    "def getDocumentsWithWord(term):\n",
    "    docs = []\n",
    "    id = id2word[term]\n",
    "    i = 0\n",
    "    for col in file.col:\n",
    "        if col == id:\n",
    "            docs.append(file.row[i])\n",
    "        i += 1\n",
    "    return docs\n",
    "\n",
    "# Retrieves all the words along with their TFIDF values in the document with id docId\n",
    "def getWordsAndTFIDF(docId):\n",
    "    data = []\n",
    "    i = 0\n",
    "    for row in file.row:\n",
    "        if docId == row:\n",
    "            data.append([file.col[i], file.data[i]])\n",
    "        i+=1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now find all the relevant words. After finding all the words in the documents that contain the input term, I sum their TFIDF values and sort the hash based on their TFIDF values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    docs = getDocumentsWithWord(inputWord)\n",
    "    parents.append(inputWord)\n",
    "\n",
    "    relevant_words = {}\n",
    "\n",
    "    for doc in docs:\n",
    "        data = getWordsAndTFIDF(doc)\n",
    "        for d in data:\n",
    "            if word2id[d[0]] in parents:\n",
    "                continue\n",
    "            if not d[0] in relevant_words:\n",
    "                relevant_words[d[0]] = 0.0\n",
    "            relevant_words[d[0]] += d[1]\n",
    "\n",
    "    from operator import itemgetter\n",
    "    sorted_relevant_words = sorted(relevant_words.items(), key=itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find second level of relevant words, I will apply the same logic to all of the top \"n\" relevant words. That is I will find the top \"n\" relevant words for all the top \"n\" relevant words of the input term. This logic can continue based on the maximum number of levels (depth) of relevant words. For now I will be calculating 4 levels of relevant words and top 10 words in each level. \n",
    "To implement this logic and create the json response, I will use recursion.\n",
    "The json is created keeping in mind the format required for the chosen visualization framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    topic = {}\n",
    "    topic['name'] = inputWord\n",
    "    topic['words'] = []\n",
    "    topic['children'] = []\n",
    "\n",
    "    for w in sorted_relevant_words[:max_words]:\n",
    "        topic['words'].append(word2id[w[0]])\n",
    "        parents.append(word2id[w[0]])\n",
    "\n",
    "    for w in sorted_relevant_words[:max_words]:\n",
    "        if level == max_level:\n",
    "            continue\n",
    "        else:\n",
    "            topic['children'].append(generateJSON(word2id[w[0]], level=level+1, parents=parents))\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Since the json is ready, I can now visualize the json. The visualization app was created based on the open source repository provided by the framework. It is a simple angular js app with one controller and one route. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "This was a great project to learn about text mining and topic modelling. \n",
    "I gained a lot of knowledge by reading several papers and online content about topic modelling. \n",
    "\n",
    "Finding ways to optimize the dictionary and corpus by \n",
    "1. removing the common words (stop words), \n",
    "2. words that appear only once, \n",
    "3. words that appear in less than 20 documents, \n",
    "4. words that appear in more than 10% of documents, \n",
    "5. lemmatizing the dictionary (finding the lemma of any word and keeping only the lemma)\n",
    "was a big learning process. \n",
    "\n",
    "Experimenting with LDA, LSA and finally choosing TFIDF was again a learning process. The difficulty in implementing and understanding the output of LDA and LSA was the main reason to choose simple TFIDF implementation. \n",
    "\n",
    "Visualization libraries was new and nice to know. They provide a wonderful way for visualizing topic modelling. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
